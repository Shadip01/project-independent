{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"instal\" - maybe you meant \"install\"\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip instal pandas numpy tensorflow \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Mappings:\n",
      "0: 2010-01\n",
      "1: 2010-02\n",
      "2: 2010-03\n",
      "3: 2010-04\n",
      "4: 2010-05\n",
      "5: 2010-06\n",
      "6: 2010-07\n",
      "7: 2010-08\n",
      "8: 2010-09\n",
      "9: 2010-10\n",
      "10: 2010-11\n",
      "11: 2010-12\n",
      "12: 2011-01\n",
      "13: 2011-02\n",
      "14: 2011-03\n",
      "15: 2011-04\n",
      "16: 2011-05\n",
      "17: 2011-06\n",
      "18: 2011-07\n",
      "19: 2011-08\n",
      "20: 2011-09\n",
      "21: 2011-10\n",
      "22: 2011-11\n",
      "23: 2011-12\n",
      "24: 2012-01\n",
      "25: 2012-02\n",
      "26: 2012-03\n",
      "27: 2012-04\n",
      "28: 2012-05\n",
      "29: 2012-06\n",
      "30: 2012-07\n",
      "31: 2012-08\n",
      "32: 2012-09\n",
      "33: 2012-10\n",
      "34: 2012-11\n",
      "35: 2012-12\n",
      "36: 2013-01\n",
      "37: 2013-02\n",
      "38: 2013-03\n",
      "39: 2013-04\n",
      "40: 2013-05\n",
      "41: 2013-06\n",
      "42: 2013-07\n",
      "43: 2013-08\n",
      "44: 2013-09\n",
      "45: 2013-10\n",
      "46: 2013-11\n",
      "47: 2013-12\n",
      "48: 2014-01\n",
      "49: 2014-02\n",
      "50: 2014-03\n",
      "51: 2014-04\n",
      "52: 2014-05\n",
      "53: 2014-06\n",
      "54: 2014-07\n",
      "55: 2014-08\n",
      "56: 2014-09\n",
      "57: 2014-10\n",
      "58: 2014-11\n",
      "59: 2014-12\n",
      "60: 2015-01\n",
      "61: 2015-02\n",
      "62: 2015-03\n",
      "63: 2015-04\n",
      "64: 2015-05\n",
      "65: 2015-06\n",
      "66: 2015-07\n",
      "67: 2015-08\n",
      "68: 2015-09\n",
      "69: 2015-10\n",
      "70: 2015-11\n",
      "71: 2015-12\n",
      "72: 2016-01\n",
      "73: 2016-02\n",
      "74: 2016-03\n",
      "75: 2016-04\n",
      "76: 2016-05\n",
      "77: 2016-06\n",
      "78: 2016-07\n",
      "79: 2016-08\n",
      "80: 2016-09\n",
      "81: 2016-10\n",
      "82: 2016-11\n",
      "83: 2016-12\n",
      "84: 2017-01\n",
      "85: 2017-02\n",
      "86: 2017-03\n",
      "87: 2017-04\n",
      "88: 2017-05\n",
      "89: 2017-06\n",
      "90: 2017-07\n",
      "91: 2017-08\n",
      "92: 2017-09\n",
      "93: 2017-10\n",
      "94: 2017-11\n",
      "95: 2017-12\n",
      "96: 2018-01\n",
      "97: 2018-02\n",
      "98: 2018-03\n",
      "99: 2018-04\n",
      "100: 2018-05\n",
      "101: 2018-06\n",
      "102: 2018-07\n",
      "103: 2018-08\n",
      "104: 2018-09\n",
      "105: 2018-10\n",
      "106: 2018-11\n",
      "107: 2018-12\n",
      "108: 2019-01\n",
      "109: 2019-02\n",
      "110: 2019-03\n",
      "111: 2019-04\n",
      "112: 2019-05\n",
      "113: 2019-06\n",
      "114: 2019-07\n",
      "115: 2019-08\n",
      "116: 2019-09\n",
      "117: 2019-10\n",
      "118: 2019-11\n",
      "119: 2019-12\n",
      "120: 2020-01\n",
      "121: 2020-02\n",
      "122: 2020-03\n",
      "123: 2020-04\n",
      "124: 2020-05\n",
      "125: 2020-06\n",
      "126: 2020-07\n",
      "127: 2020-08\n",
      "128: 2020-09\n",
      "129: 2020-10\n",
      "130: 2020-11\n",
      "131: 2020-12\n",
      "132: 2021-01\n",
      "133: 2021-02\n",
      "134: 2021-03\n",
      "135: 2021-04\n",
      "136: 2021-05\n",
      "137: 2021-06\n",
      "138: 2021-07\n",
      "139: 2021-08\n",
      "140: 2021-09\n",
      "141: 2021-10\n",
      "142: 2021-11\n",
      "143: 2021-12\n",
      "144: 2022-01\n",
      "145: 2022-02\n",
      "146: 2022-03\n",
      "147: 2022-04\n",
      "148: 2022-05\n",
      "149: 2022-06\n",
      "150: 2022-07\n",
      "151: 2022-08\n",
      "152: 2022-09\n",
      "153: 2022-10\n",
      "154: 2022-11\n",
      "155: 2022-12\n",
      "156: 2023-01\n",
      "157: 2023-02\n",
      "158: 2023-03\n",
      "159: 2023-04\n",
      "160: 2023-05\n",
      "161: 2023-06\n",
      "162: 2023-07\n",
      "163: 2023-08\n",
      "164: 2023-09\n",
      "165: 2023-10\n",
      "166: 2023-11\n",
      "167: 2023-12\n",
      "168: 2024-01\n",
      "169: 2024-02\n",
      "170: 2024-03\n",
      "171: 2024-04\n",
      "172: 2024-05\n",
      "173: 2024-06\n",
      "174: 2024-07\n",
      "175: 2024-08\n",
      "176: 2024-09\n",
      "177: 2024-10\n"
     ]
    }
   ],
   "source": [
    "file_name = '../DATASETS/RNNs-datasets/data_reports_monthly.csv'\n",
    "database = pd.read_csv(file_name)\n",
    "\n",
    "database.replace('-', np.nan, inplace=True)  # Replace '-' with NaN\n",
    "database = database.dropna(subset=['Trips Per Day', 'Unique Vehicles', 'Unique Drivers', 'License Class'])\n",
    "database['Trips Per Day'] = database['Trips Per Day'].replace({',': ''}, regex=True).astype(float)\n",
    "database['Unique Vehicles'] = database['Unique Vehicles'].replace({',': ''}, regex=True).astype(float)\n",
    "database['Unique Drivers'] = database['Unique Drivers'].replace({',': ''}, regex=True).astype(float)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "database['License Class'] = encoder.fit_transform(database['License Class'])\n",
    "\n",
    "encoder.fit(database['Month/Year']) \n",
    "\n",
    "print(\"Correct Mappings:\")\n",
    "for encoded, original in enumerate(encoder.classes_):\n",
    "    print(f\"{encoded}: {original}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 7  # Number of past rows to consider for each sequence\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(features, target, timesteps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(features) - timesteps):\n",
    "        X.append(features[i : i + timesteps])  \n",
    "        y.append(target[i + timesteps])      \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Extract features and target\n",
    "features = database[['Trips Per Day', 'Unique Vehicles', 'Unique Drivers', 'License Class']].values\n",
    "target = database['Month/Year'].values\n",
    "\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "target_encoded = encoder.fit_transform(target)\n",
    "num_classes = len(encoder.classes_)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Create sequences\n",
    "X, y = create_sequences(features_scaled, target_encoded, timesteps)\n",
    "\n",
    "# One-hot encode the target for classification\n",
    "y = to_categorical(y, num_classes=num_classes)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "y_test_classes = np.argmax(y_test, axis=1)  \n",
    "y_pred_classes = np.argmax(y_test, axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 64)                17664     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19777 (77.25 KB)\n",
      "Trainable params: 19777 (77.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=(timesteps, X_train.shape[2])),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear') \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "16/16 [==============================] - 1s 10ms/step - loss: 0.0061 - mae: 0.0238 - val_loss: 0.0056 - val_mae: 0.0101\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0058 - mae: 0.0174 - val_loss: 0.0056 - val_mae: 0.0074\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0057 - mae: 0.0140 - val_loss: 0.0056 - val_mae: 0.0116\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0125 - val_loss: 0.0056 - val_mae: 0.0125\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0128 - val_loss: 0.0056 - val_mae: 0.0100\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0120 - val_loss: 0.0056 - val_mae: 0.0101\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0116 - val_loss: 0.0056 - val_mae: 0.0107\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0115 - val_loss: 0.0056 - val_mae: 0.0114\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0116 - val_loss: 0.0056 - val_mae: 0.0103\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0113 - val_loss: 0.0056 - val_mae: 0.0106\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0114 - val_loss: 0.0056 - val_mae: 0.0108\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0113 - val_loss: 0.0056 - val_mae: 0.0113\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0113 - val_loss: 0.0056 - val_mae: 0.0107\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0114 - val_loss: 0.0056 - val_mae: 0.0109\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0113 - val_loss: 0.0056 - val_mae: 0.0114\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0112 - val_loss: 0.0056 - val_mae: 0.0114\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0112 - val_loss: 0.0056 - val_mae: 0.0109\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0112 - val_loss: 0.0056 - val_mae: 0.0117\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0112 - val_loss: 0.0056 - val_mae: 0.0116\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0112 - val_loss: 0.0056 - val_mae: 0.0116\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0113 - val_loss: 0.0056 - val_mae: 0.0108\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0110 - val_loss: 0.0056 - val_mae: 0.0112\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0112 - val_loss: 0.0056 - val_mae: 0.0117\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0112 - val_loss: 0.0056 - val_mae: 0.0115\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0112 - val_loss: 0.0056 - val_mae: 0.0111\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0111 - val_loss: 0.0056 - val_mae: 0.0115\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0113 - val_loss: 0.0056 - val_mae: 0.0113\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0056 - mae: 0.0111 - val_loss: 0.0056 - val_mae: 0.0119\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0113 - val_loss: 0.0056 - val_mae: 0.0109\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0110 - val_loss: 0.0056 - val_mae: 0.0118\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 922us/step - loss: 0.0056 - mae: 0.0118\n",
      "Test Loss: 0.0055870478972792625, Test MAE: 0.01180651132017374\n"
     ]
    }
   ],
   "source": [
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One-hot encode the target if it's categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Update output layer\n",
    "model.add(Dense(64, activation='softmax'))  \n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 934us/step\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_month(features):\n",
    "    # Ensure input is a 3D array (timesteps, features)\n",
    "    features = np.array(features)\n",
    "    if features.shape != (timesteps, features.shape[1]):\n",
    "        raise ValueError(f\"Input must have shape ({timesteps}, {features.shape[1]}): {timesteps} timesteps of {features.shape[1]} features each.\")\n",
    "    \n",
    "    # Flatten the features to 2D for MinMaxScaler\n",
    "    features_flat = features.reshape(-1, features.shape[-1])  # Shape (timesteps, features)\n",
    "    \n",
    "    # Scale the features\n",
    "    features_scaled = scaler.transform(features_flat)  # MinMaxScaler expects (n_samples, n_features)\n",
    "    \n",
    "    # Reshape back to 3D for the model\n",
    "    features_reshaped = features_scaled.reshape((1, timesteps, features.shape[-1]))  # Shape (1, timesteps, features)\n",
    "    \n",
    "    # Predict using the model\n",
    "    prediction = model.predict(features_reshaped)\n",
    "    \n",
    "    # Get the predicted class\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    \n",
    "    # Decode the predicted class to the original label\n",
    "    return encoder.inverse_transform([predicted_class])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step\n",
      "Predicted Month/Year: 2014-05\n"
     ]
    }
   ],
   "source": [
    "# Example input: 7 timesteps, each with 4 features\n",
    "example_features = [\n",
    "    [50, 10, 8, 3],  # Timestep 1\n",
    "    [52, 12, 9, 4],  # Timestep 2\n",
    "    [48, 11, 7, 3],  # Timestep 3\n",
    "    [49, 9, 8, 2],   # Timestep 4\n",
    "    [51, 10, 9, 3],  # Timestep 5\n",
    "    [50, 11, 8, 3],  # Timestep 6\n",
    "    [49, 10, 8, 3]   # Timestep 7\n",
    "]\n",
    "\n",
    "# Predict the \"Month/Year\"\n",
    "predicted_month = predict_month(example_features)\n",
    "print(f\"Predicted Month/Year: {predicted_month}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
